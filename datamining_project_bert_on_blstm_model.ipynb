{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd237f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_label_train(tags, id2tag):\n",
    "    labels = []\n",
    "    for tag in tags:\n",
    "        pre = [id2tag.get(str(int(_id))) for _id in tag]\n",
    "        labels.append(pre)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def recover_label(tags, id2tag):\n",
    "    labels = []\n",
    "    for tag in tags:\n",
    "        pre = [id2tag.get(str(_id), 'O') for _id in tag]\n",
    "        labels.append(pre)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def recover_label_test(tags, id2tag, sens_len):\n",
    "    labels = []\n",
    "    #print(id2tag.get(str(12)), 'O')\n",
    "    for i in range(len(tags)):\n",
    "        tag = tags[i][:sens_len[i]]\n",
    "        pre = [id2tag.get(str(int(_id))) for _id in tag]\n",
    "        labels.append(pre)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def recover_bert_label(tags, id2tag):\n",
    "    labels = []\n",
    "    for tag in tags:\n",
    "        pre = [id2tag.get(str(_id), 'O') for _id in tag[1:-1]]\n",
    "        labels.append(pre)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_ner_fmeasure(golden_lists, predict_lists, label_type=\"BMES\"):\n",
    "    golden_full = []\n",
    "    predict_full = []\n",
    "    right_full = []\n",
    "    right_tag = 0\n",
    "    all_tag = 0\n",
    "    for idx, (golden_list, predict_list) in enumerate(zip(golden_lists, predict_lists)):\n",
    "        for golden_tag, predict_tag in zip(golden_list, predict_list):\n",
    "            if golden_tag == predict_tag:\n",
    "                right_tag += 1\n",
    "        all_tag += len(golden_list)\n",
    "        if label_type == \"BMES\":\n",
    "            gold_matrix = get_ner_BMES(golden_list)\n",
    "            pred_matrix = get_ner_BMES(predict_list)\n",
    "        else:\n",
    "            gold_matrix = get_ner_BIO(golden_list)\n",
    "            pred_matrix = get_ner_BIO(predict_list)\n",
    "        right_ner = list(set(gold_matrix).intersection(set(pred_matrix)))\n",
    "        golden_full += gold_matrix\n",
    "        predict_full += pred_matrix\n",
    "        right_full += right_ner\n",
    "    right_num = len(right_full)\n",
    "    golden_num = len(golden_full)\n",
    "    predict_num = len(predict_full)\n",
    "    if predict_num == 0:\n",
    "        precision = -1\n",
    "    else:\n",
    "        precision = (right_num + 0.0) / predict_num\n",
    "    if golden_num == 0:\n",
    "        recall = -1\n",
    "    else:\n",
    "        recall = (right_num + 0.0) / golden_num\n",
    "    if (precision == -1) or (recall == -1) or (precision + recall) <= 0.:\n",
    "        f_measure = -1\n",
    "    else:\n",
    "        f_measure = 2 * precision * recall / (precision + recall)\n",
    "    accuracy = (right_tag + 0.0) / all_tag\n",
    "    print(\"gold_num = \", golden_num, \" pred_num = \", predict_num, \" right_num = \", right_num)\n",
    "    return round(accuracy, 4), round(precision, 4), round(recall, 4), round(f_measure, 4)\n",
    "\n",
    "\n",
    "def reverse_style(input_string):\n",
    "    target_position = input_string.index('[')\n",
    "    input_len = len(input_string)\n",
    "    output_string = input_string[target_position:input_len] + input_string[0:target_position]\n",
    "    return output_string\n",
    "\n",
    "\n",
    "def get_ner_BMES(label_list):\n",
    "    list_len = len(label_list)\n",
    "    begin_label = 'B'\n",
    "    end_label = 'E'\n",
    "    single_label = 'S'\n",
    "    whole_tag = ''\n",
    "    index_tag = ''\n",
    "    tag_list = []\n",
    "    stand_matrix = []\n",
    "    for i in range(list_len):\n",
    "        # wordlabel = word_list[i]\n",
    "        current_label = label_list[i].upper()\n",
    "        tags = current_label.split('-')\n",
    "        if begin_label in current_label:\n",
    "            if index_tag != '':\n",
    "                tag_list.append(whole_tag + ',' + str(i - 1))\n",
    "            whole_tag = tags[-1] + '[' + str(i)\n",
    "            index_tag = tags[-1]\n",
    "\n",
    "        elif single_label in current_label:\n",
    "            if index_tag != '':\n",
    "                tag_list.append(whole_tag + ',' + str(i - 1))\n",
    "            whole_tag = tags[-1] + '[' + str(i)\n",
    "            tag_list.append(whole_tag)\n",
    "            whole_tag = \"\"\n",
    "            index_tag = \"\"\n",
    "        elif end_label in current_label:\n",
    "            if index_tag != '':\n",
    "                tag_list.append(whole_tag + ',' + str(i))\n",
    "            whole_tag = ''\n",
    "            index_tag = ''\n",
    "        else:\n",
    "            continue\n",
    "    if (whole_tag != '') & (index_tag != ''):\n",
    "        tag_list.append(whole_tag)\n",
    "    tag_list_len = len(tag_list)\n",
    "\n",
    "    for i in range(0, tag_list_len):\n",
    "        if len(tag_list[i]) > 0:\n",
    "            tag_list[i] = tag_list[i] + ']'\n",
    "            insert_list = reverse_style(tag_list[i])\n",
    "            stand_matrix.append(insert_list)\n",
    "    # print stand_matrix\n",
    "    return stand_matrix\n",
    "\n",
    "\n",
    "def get_ner_BIO(label_list):\n",
    "    list_len = len(label_list)\n",
    "    begin_label = 'B-'\n",
    "    inside_label = 'I-'\n",
    "    whole_tag = ''\n",
    "    index_tag = ''\n",
    "    tag_list = []\n",
    "    stand_matrix = []\n",
    "    for i in range(0, list_len):\n",
    "        # wordlabel = word_list[i]\n",
    "        current_label = label_list[i].upper()\n",
    "        if begin_label in current_label:\n",
    "            if index_tag == '':\n",
    "                whole_tag = current_label.replace(begin_label, \"\", 1) + '[' + str(i)\n",
    "                index_tag = current_label.replace(begin_label, \"\", 1)\n",
    "            else:\n",
    "                tag_list.append(whole_tag + ',' + str(i - 1))\n",
    "                whole_tag = current_label.replace(begin_label, \"\", 1) + '[' + str(i)\n",
    "                index_tag = current_label.replace(begin_label, \"\", 1)\n",
    "\n",
    "        elif inside_label in current_label:\n",
    "            if current_label.replace(inside_label, \"\", 1) == index_tag:\n",
    "                whole_tag = whole_tag\n",
    "            else:\n",
    "                if (whole_tag != '') & (index_tag != ''):\n",
    "                    tag_list.append(whole_tag + ',' + str(i - 1))\n",
    "                whole_tag = ''\n",
    "                index_tag = ''\n",
    "        else:\n",
    "            if (whole_tag != '') & (index_tag != ''):\n",
    "                tag_list.append(whole_tag + ',' + str(i - 1))\n",
    "            whole_tag = ''\n",
    "            index_tag = ''\n",
    "\n",
    "    if (whole_tag != '') & (index_tag != ''):\n",
    "        tag_list.append(whole_tag)\n",
    "    tag_list_len = len(tag_list)\n",
    "\n",
    "    for i in range(0, tag_list_len):\n",
    "        if len(tag_list[i]) > 0:\n",
    "            tag_list[i] = tag_list[i] + ']'\n",
    "            insert_list = reverse_style(tag_list[i])\n",
    "            stand_matrix.append(insert_list)\n",
    "    return stand_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "140cc272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Bertencoder(nn.Module):\n",
    "    def __init__(self, bert_dim, output_dim, num_layers, rnn_dim):\n",
    "        super(Bertencoder, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.rnn = nn.LSTM(bert_dim, rnn_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(rnn_dim*2, output_dim)\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, y, seg_ids, mask):\n",
    "        bert_output = self.bert_model(x, token_type_ids=seg_ids, attention_mask=mask)\n",
    "        lstm_output, _ = self.rnn(bert_output.last_hidden_state)\n",
    "        output = self.linear(lstm_output).transpose(1, 2)\n",
    "        loss = self.loss_function(output, y)\n",
    "        return loss\n",
    "\n",
    "    def test(self, x, y, seg_ids, mask):\n",
    "\n",
    "        bert_output = self.bert_model(x, token_type_ids=seg_ids, attention_mask=mask)\n",
    "        lstm_output, _ = self.rnn(bert_output.last_hidden_state)\n",
    "        output = self.linear(lstm_output).transpose(1, 2)\n",
    "        loss = self.loss_function(output, y)\n",
    "        predict = self.predict_label(output.transpose(1, 2), mask)\n",
    "        return loss, predict\n",
    "\n",
    "    def predict_label(self, output, mask):\n",
    "        \"\"\"\n",
    "        output: batch_size, max_sentence_len, tag_num\n",
    "        mask: same label data\n",
    "        batch_label: batch_size, sentence_label\n",
    "        \"\"\"\n",
    "        pre_label = []\n",
    "        pre_label_pad = output.argmax(2)\n",
    "        for i in range(mask.shape[0]):\n",
    "            sent_label = []\n",
    "            for j in range(mask.shape[1]):\n",
    "                if mask[i, j] == 1:\n",
    "                    sent_label.append(pre_label_pad[i, j].item())\n",
    "            sent_label = sent_label[1:len(sent_label)-1]\n",
    "            pre_label.append(sent_label)\n",
    "\n",
    "        return pre_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5a57538",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_labels = {',': 'I-COMMA',\n",
    "                  '.': 'I-DOT',\n",
    "                  '?': 'I-QMARK',\n",
    "                  '!': 'I-EMARK',\n",
    "                  ':': 'I-COLON',\n",
    "                  ';': 'I-SEMICOLON'}\n",
    "normal_label = 'O'\n",
    "labels_set = list(special_labels.values()) + [normal_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39574688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d2d90b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_args = NERArgs()\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.save_steps = -1\n",
    "ner_model = NERModel('bert',\n",
    "                 'bert-base-uncased',\n",
    "                 labels = labels_set,\n",
    "                 args=model_args,\n",
    "                 use_cuda=False)\n",
    "train_df = pd.read_csv('./wikitext/train0-10.csv').dropna()\n",
    "validation_df = pd.read_csv('./wikitext/validation.csv').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23966ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4893d9cfa75440a1afc94c50d0c93c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6627d04a8946e78b1894b19613a427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = ner_model.load_and_cache_examples(train_df)\n",
    "validation_dataset = ner_model.load_and_cache_examples(validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66aea700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106953, 2285)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e34601dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fe606750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam, SGD\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "bert_dim = 768\n",
    "output_dim = len(labels_set)\n",
    "batch_size = 32\n",
    "adam_lr = 3e-5\n",
    "epoch = 1\n",
    "num_layers = 1\n",
    "rnn_dim = 512\n",
    "\n",
    "def train(tr_ds, val_ds):\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    device = torch.device('cuda:0' if use_gpu else 'cpu')\n",
    "    train_dataloader = DataLoader(tr_ds, batch_size=batch_size, shuffle=False)\n",
    "    validation_dataloader = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = Bertencoder(bert_dim, output_dim, num_layers, rnn_dim)\n",
    "    if use_gpu:\n",
    "        model.to(device)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=adam_lr)\n",
    "\n",
    "    # train model\n",
    "    for i in range(epoch):\n",
    "        epoch_loss = 0\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Running Epoch {i+1} of {epoch}\", mininterval=0)\n",
    "        \n",
    "        for (j, batch) in enumerate(batch_iterator):\n",
    "            batch_x = batch[0].to(device)\n",
    "            batch_y = batch[3].to(device)\n",
    "            batch_seg = batch[2].to(device)\n",
    "            batch_msk = batch[1].to(device)\n",
    "            #loss = model(train_x[j], train_y[j], train_seg_ids[j], train_mask[j]).mean()                # dataparallel\n",
    "            loss = model(batch_x, batch_y, batch_seg, batch_msk)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        # print(\"epoch_loss:\\t{}\".format(round(epoch_loss/(j+1), 4)))\n",
    "        print(f\"Epochs {i}/{epoch}. Running Loss: {epoch_loss/(j+1):9.4f}\")\n",
    "#         batch_iterator.set_description(f\"Epochs {i}/{epoch}. Running Loss: {epoch_loss/(j+1):9.4f}\")\n",
    "\n",
    "#         # test model\n",
    "#         dev_loss = 0\n",
    "#         predict_idx_label = []\n",
    "\n",
    "#         for (h, val_batch) in enumerate(validation_dataloader):\n",
    "#             val_batch_x = val_batch[0].to(device)\n",
    "#             val_batch_y = val_batch[3].to(device)\n",
    "#             val_batch_seg = val_batch[2].to(device)\n",
    "#             val_batch_msk = val_batch[1].to(device)\n",
    "#             with torch.no_grad():\n",
    "#                 #loss, pre_y = model.module.test(test_x[h], test_y[h], test_seg_ids[h], test_mask[h])     # dataparallel\n",
    "#                 loss_val, pre_y = model.test(batch_x, batch_y, batch_seg, batch_msk)\n",
    "#             #dev_loss += loss.mean().item()        # dataparallel\n",
    "#             dev_loss += loss_val.item()\n",
    "\n",
    "#             predict_idx_label.append(pre_y)\n",
    "#             #print('test_iteration:\\t{}\\t\\tloss:\\t{}'.format(h+1, round(loss.mean().item(), 4)))          # dataparallel\n",
    "# #             print('test_iteration:\\t{}\\t\\tloss:\\t{}'.format(h+1, round(loss.item(), 4)))\n",
    "\n",
    "# #         predict_label = test_data.trans2tag(predict_idx_label)\n",
    "\n",
    "# #         a, p, r, f = get_ner_fmeasure(label_y, predict_label)\n",
    "#         print('train_loss:\\t{}\\t\\tdev_loss:\\t{}'.format(round(epoch_loss/(j+1), 4), round(dev_loss/(h+1), 4)))\n",
    "# #         print('A:\\t{}\\t\\tP:\\t{}\\t\\tR:\\t{}\\t\\tF:\\t{}'.format(a, p, r, f))\n",
    "#         print('*******************************************************')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d1f8ccb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5257dcfa10cf4f6aaf13ce4ee3980b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 1:   0%|          | 0/3343 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = train(train_dataset, validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d94c3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bertencoder(\n",
      "  (bert_model): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (rnn): LSTM(768, 512, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=1024, out_features=7, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaa2521",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
